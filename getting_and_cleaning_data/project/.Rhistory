#3.6.3 Multiple Linear Regression
# fit the linear model with 2 predictors (lstat, age)
lm.fit <- lm(medv~lstat+age, data=Boston)
summary(lm.fit)
#fit all the predictors in the Boston dataset using shorthand
lm.fit <- lm(medv~., data=Boston)
summary(lm.fit)
?summary.lm
summary(lm.fit)$r.sq #or $r.squared
summary(lm.fit)$sigma  #RSE
#vif = Variance Inflation Factors
#install.packages("car")
library(car)
vif(lm.fit)
#perform regression for all variables except age
lm.fit1 <- lm(medv~.-age, data=Boston)
summary(lm.fit1)
# or use the update function
lm.fit1 <- update(lm.fit,~.-age)
summary(lm.fit1)
#3.6.4 Interaction Terms
# syntax term1:term2 creates an interaction term in a linear model
# syntax term1*term2 does term1, term2 and term1xterm2 in model
#   shorthand for term1+term2+term1:term2
summary(lm(medv~lstat*age,data=Boston))
#3.6.5 Non-linear Transformations of the Predictors
lm.fit1
?I
lm.fit2 <- lm(medv~lstat+I(lstat^2))
attach(Boston)
library(MASS)
library(ISLR)
#install.packages("ISLR")
attach(Boston)
library(car)
lm.fit2 <- lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
summary(lm.fit1)
vif(lm.fit)
#perform regression for all variables except age
lm.fit1 <- lm(medv~.-age, data=Boston)
summary(lm.fit1)
# or use the update function
lm.fit1 <- update(lm.fit,~.-age)
summary(lm.fit1)
summary(lm.fit2)  #the near 0 p-value suggests a better fit
lm.fit <- lm(medv~lstat)
anova(lm.fit, lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
?poly
lm.fit5 <- lm(medv~poly(lstat, 5))
summary(lm.fit5)
summary(lm(medv~log(rm), data=Boston))
fix(Carseats)
names(Carseats)
lm.fit <- lm(Sales~.+Income:Advertising+Price:Age, data=Carseats)
summary(lm.fit)
?contrasts
attach(Carseats)
contrasts(ShelveLoc)
LoadLibraries <- function() {}
LoadLibraries <- function() {
library(ISLR)
library(MASS)
print("The libraries have been loaded")
}
LoadLibraries
LoadLibraries() #runs the function
save.image("C:/ds/Dropbox/Reference/ebooks/for_book_club/ISLR_Materials/ISLR_ch3_workspace.RData")
Auto
library(ISLR)
Auto
Attach(Auto)
lm.mpg.hp <- lm(mpg~hp)
names(Auto)
lm.mpg.hp <- lm(mpg~horsepower)
attach(Auto)
names(Auto)
lm.mpg.hp <- lm(mpg~horsepower)
summary(lm.mpg.hp)
plot(mpg, horsepower)
abline(lm.mpg.hp, lwd="3", col="red")
plot(horsepower, mpg)
abline(lm.mpg.hp, lwd="3", col="red")
# it appears to be strong since the std err is small (.006446)
mean(mpg)
RSE(lm.mpg.hp)
names(lm.mpg.hp)
residuals(lm.mpg.hp)
summary(lm.mpg.hp)
rse <- 4.906
rse/mean(mpg)
r.sq(lm.mpg.hp)
predict(lm.mpg.hp, data.frame(horsepower=c(98)), interval="confidence")
predict(lm.mpg.hp, data.frame(horsepower=c(98)), interval="prediction")
plot(horsepower, mpg)
abline(lm.mpg.hp, lwd="3", col="red")
par=(mfrow=c(2,2))
plot(lm.mpg.hp)
rm(par)
par(mfrow=c(2,2))
plot(lm.mpg.hp)
pairs(Auto)
cor(subset(Auto, select-=name))
cor(subset(Auto, select=-name
cor(subset(Auto, select=-name)
cor(subset(Auto, select=-name)
cor(subset(Auto, select=-name))
cor(subset(Auto, select=-name))
lm.fit <- lm(mpg~.-name)
lm.fit <- lm(mpg~.-name, data=Auto)
summary(lm.fit)
?paste
8 %% 2
?read.table
cities <- c("New York", "Paris", "London", "Tokyo", "Rio de Janeiro", "Cape Town")
lapply(cities, nchar)
unlist(lapply(cities, nchar))
?strsplit
?subset
?sort
?seq
?subset
?strptime
version
set.seed(1)
rpois(5, 2)
?rnorm
?qpois
?dpois
set.seed(10)
x <- rep(0:1, each = 5)
e <- rnorm(10, 0, 20)
y <- 0.5 + 2 * x + e
x
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
source('~/.active-rstudio-document', echo=TRUE)
c = rbind(c(1,2,3), c(0, 1, 4), c(5,6,0))
yy <- makeCacheMatrix(c)
cacheSolve(yy)
cacheSolve(yy)
## Matrix inversion is usually a costly computation and there may be some
## benefit to caching the inverse of a matrix rather than compute it repeatedly.
## This is a pair of functions that cache the inverse of a matrix.
##This function creates a special "matrix" object that can cache its inverse.
makeCacheMatrix <- function(x = matrix()) {
m <- NULL
set <- function(y) {  ## set the value of the vector
x <<- y
m <<- NULL
}
get <- function() x   ##get the value of the vector
setinverse <- function(solve) m <<- solve  ##set the value of the inverse
getinverse <- function() m   ##get the value of the inverse
list(set = set, get = get,
setinverse = setinverse,
getinverse = getinverse)
}
## This function computes the inverse of the special "matrix" returned by
## makeCacheMatrix above. If the inverse has already been calculated
## (and the matrix has not changed), then the cachesolve should retrieve
## the inverse from the cache.
cacheSolve <- function(x, ...) {
## Return a matrix that is the inverse of 'x'
m <- x$getinverse()
if(!is.null(m)) {    ##decide if there is a cached result
message("getting cached data")
print(m)
message("finished getting cached data")
}else{ ##when there is no cached result, calculate it and return the result
data <- x$get()
m <- solve(data,...)
x$setinverse(m)
m
}
}
c = rbind(c(1,2,3), c(0, 1, 4), c(5,6,0))
yy <- makeCacheMatrix(c)
m2 <- cacheSolve(yy)
m2
solve(m2,)
m3 <- cacheSolve(yy)
m3
## Put comments here that give an overall description of what your
## functions do
## This function creates a special "matrix" object that can cache its inverse.
makeCacheMatrix <- function(x = matrix()) {
# Setting cache to NULL
inverseCached <<- NULL
# Setting matrix entry
matrixEntry <<- x
# Inversing the matrix and putting it in the cache
inverseCached <<- cacheSolve(x)
# Returning the matrix
x
}
## This function computes the inverse of the special "matrix" returned by makeCacheMatrix
## If the inverse has already been calculated (and the matrix has not changed),
## then cacheSolve should retrieve the inverse from the cache.
cacheSolve <- function(x) {
## Return a matrix that is the inverse of 'x'
# Check if matrix to be inversed is the same that was cached in the past
if(isTRUE(all(matrixEntry == x))) {
# If same matrix, then check if cache is null or not to retreive inverse if not null
if(!is.null(inverseCached)) {
message("getting cached inversed matrix")
return(inverseCached)
}
}
# if not same matrix or not cached, then return the result of matrix inversion
# and cache the result for next usage if needed
message("getting the else")
inverseCached <<- solve(x)
inverseCached
}
c = rbind(c(1,2,3), c(0, 1, 4), c(5,6,0))
yy <- makeCacheMatrix(c)
m2 <- cacheSolve(yy)
m2
m3 <- cacheSolve(yy)
source('~/.active-rstudio-document', echo=TRUE)
iris %>% ggvis(x = ~Sepal.Width,
y = ~Sepal.Length,
fill = ~Species,
size = ~Petal.Width) %>%
layer_points()
iris %>% ggvis(x = ~Sepal.Width,
y = ~Sepal.Length,
fill = ~Species,
size = ~Petal.Width) %>% layer_points()
require(ggvis)
install.packages("ggvis")
require(ggvis)
iris %>% ggvis(x = ~Sepal.Width,
y = ~Sepal.Length,
fill = ~Species,
size = ~Petal.Width) %>% layer_points()
mtcars %>%
ggvis(~mpg, ~wt,
fill := input_text(label = "Choose color:", value="black")) %>%
layer_points()
mtcars %>%
ggvis(~mpg, ~wt, fill=input_select(label="Choose fill variable:",
choices = names(mtcars),
map=as.name)) %>%
layer_points()
mtcars %>%
ggvis(~mpg, width=input_numeric(label="Choose a bindwidth:",
value=1)) %>%
layer_histograms(width = 2)
mtcars %>%
ggvis(~mpg, ~wt, fill=input_select(label="Choose fill variable:",
choices = names(mtcars),
map=as.name)) %>%
layer_points()
library("dplyr", lib.loc="~/R/win-library/3.1")
library("ggvis", lib.loc="~/R/win-library/3.1")
mtcars %>%
ggvis(~mpg, ~wt, fill=input_select(label="Choose fill variable:",
choices = names(mtcars),
map=as.name)) %>%
layer_points()
faithful %>% compute_bin(~waiting, width=5) %>% ggvis(x=~xmin_, x2=~xmax_, y=0, y2=~count_) %>% layer_rects()
faithful %>% ggvis(~waiting) %>% layer_densities(fill:="green")
source('C:/ds/git/datacamp/reporting_w_R_markdown/1_R_markdown_exercise_interface.R')
install.packages("nasaweather")
source('C:/Users/Donald/OneDrive/dc_metric_stubs.R', echo=TRUE)
source('C:/Users/Donald/OneDrive/dc_metric_stubs.R', echo=TRUE)
add.column.dc_not_c_no_dc_complete_date  <- function(x)
{
}
#Computes when DC In Progress but there is a DC Complete Date
# dc_ip_has_dc_complete_date
add.column.dc_not_c_no_dc_complete_date  <- function(x)
{
}
#Computes when DC is Not Started but there is a DC Complete Date
# dc_ns_has_dc_complete_date
add.column.dc_not_c_no_dc_complete_date  <- function(x)
{
}
#Computes when DC is not complete and there is no DC Complete Date
# dc_not_c_no_dc_complete_date
add.column.dc_not_c_no_dc_complete_date  <- function(x)
{
}
source('C:/Users/Donald/OneDrive/dc_metric_stubs.R', echo=TRUE)
?runTestSuite
library(RUnit)
install.packages("RUnit")
library(RUnit)
library(RUnit)
?runTestSuite
library(RUnit)
library(RUnit)
library(dplyer)
install.packages("dplyer")
library(dplyer)
install.packages("RMySQL")
library(dplyr)
library(RMySQL)
install.packages("httr")
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github",
key = "3a3dad5b21a6bb78aa59",
secret = "6fd7a1d6fe0dac88ac195ac888939049c4588a1f")
github_token <- oath2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
install.packages("httpuv")
install.packages("httpuv")
library(httpuv)
install.packages("httpuv")
library(httpuv)
install.packages("httpuv")
library(httpuv)
library(httpuv)
setwd("C:\\ds\\git\\datasciencecoursera\\getting_and_cleaning_data\\project")
# You should create one R script called run_analysis.R that does the following.
# 1. Merges the training and the test sets to create one data set.
test.subject_test = read.table(".\\test\\subject_test.txt")
test.subject_test = read.table(".\\uci_har_dataset\\test\\subject_test.txt")
View(test.subject_test)
test.dir <- ".\\uci_har_dataset\\test\\"
train.dir <- ".\\uci_har_dataset\\train\\"
subject_test.txt <- "subject_test.txt"
x_test.txt <- "X_test.txt"
y_test.txt <- "Y_test.txt"
test.subject_test = read.table(paste0(test.dir,subject_test.txt))
test.x_test = read.table(paste0(test.dir,x_test.txt))
test.y_test <- read.table(paste0(test.dir, y_test.txt))
train.subject_test = read.table(paste0(train.dir,subject_test.txt))
train.x_test <- read.table(paste0(train.dir, x_test.txt))
setwd("C:\\ds\\git\\datasciencecoursera\\getting_and_cleaning_data\\project")
test.dir <- ".\\uci_har_dataset\\test\\"
train.dir <- ".\\uci_har_dataset\\train\\"
# You should create one R script called run_analysis.R that does the following.
# 1. Merges the training and the test sets to create one data set.
test.subject_test = read.table(paste0(test.dir,"subject_test.txt"))
test.x_test = read.table(paste0(test.dir,"x_test.txt"))
test.y_test <- read.table(paste0(test.dir, "y_test.txt"))
train.subject_test <- read.table(paste0(train.dir,"subject_train.txt"))
train.x_test <- read.table(paste0(train.dir, "x_train.txt"))
train.y_test <- read.table(paste0(train.dir, "y_train.txt"))
features.txt <- read.table(".\\uci_har_dataset\\features.txt")
View(features.txt)
View(features.txt)
View(test.y_test)
View(test.subject_test)
merged.data <- cbind(test.x_test, train.x_test)
merged.data <- rbind(test.x_test, train.x_test)
dim(test.x_test)[1] + dim(train.x_test)[1]
merged.data[,2]
features.txt[,2]
names(merged.data) <- features.txt[,2]
str(merged.data)
View(merged.data)
merged.y.data <- rbind(test.y_test, train.y_test)
setwd("C:\\ds\\git\\datasciencecoursera\\getting_and_cleaning_data\\project")
test.dir <- ".\\uci_har_dataset\\test\\"
train.dir <- ".\\uci_har_dataset\\train\\"
# You should create one R script called run_analysis.R that does the following.
# 1. Merges the training and the test sets to create one data set.
test.subject = read.table(paste0(test.dir,"subject_test.txt"))
test.x = read.table(paste0(test.dir,"x_test.txt"))
test.y <- read.table(paste0(test.dir, "y_test.txt"))
train.subject <- read.table(paste0(train.dir,"subject_train.txt"))
train.x <- read.table(paste0(train.dir, "x_train.txt"))
train.y <- read.table(paste0(train.dir, "y_train.txt"))
features.txt <- read.table(".\\uci_har_dataset\\features.txt")
merged.x.data <- rbind(test.x, train.x)
names(merged.x.data) <- features.txt[,2]
merged.y.data <- rbind(test.y, train.y)
merged.subject.data <- rbind(test.subject, train.subject)
names(merged.y.data)
names(merged.y.data) <- "y.data"
names(merged.subject.data)
names(merged.y.data) <- "y"
merged.subject.data <- rbind(test.subject, train.subject)
names(merged.subject.data) <- "subject"
merged.data <- cbind(merged.subject.data, merged.y.data, merged.x.data)
names(merged.data)
library(dplyr)
install.packages("dplyr")
merged.data <- merged.data %>% select(subject, y, grepl("mean()", names(merged.data)), grepl("std()", names(merged.data)))
library(dplyr)
merged.data <- merged.data %>% select(subject, y, grepl("mean()", names(merged.data)), grepl("std()", names(merged.data)))
grepl("mean()", names(merged.data))
merged.data <- merged.data %>% select(subject, y, grepl("mean()", names(merged.data))==TRUE, grepl("std()", names(merged.data))==TRUE)
merged.data <- merged.data[,subject, y, grepl("mean()", names(merged.data)), grepl("std()", names(merged.data))]
merged.data <- merged.data %>% select(subject, y, names(merged.data)[grepl("mean()", names(merged.data))], names(merged.data)[grepl("std()", names(merged.data))])
names(merged.data)[grepl("mean()", names(merged.data))]
merged.x.data <- merged.x.data[, grepl("mean()", names(merged.x.data)) | grepl("std()", names(merged.x.data))]
View(merged.x.data)
merged.y.data <- rbind(test.y, train.y)
names(merged.y.data) <- "y"
merged.subject.data <- rbind(test.subject, train.subject)
names(merged.subject.data) <- "subject"
merged.data <- cbind(merged.subject.data, merged.y.data, merged.x.data)
names(merged.data)
activity.labels <- read.table(".\\uci_har_dataset\\activity_labels.txt")
View(activity.labels)
names(activity.labels) <- c("activity.id, activity.name")
d <- left_join(merged.data, activity.labels, by=c("y", "activity.id"))
?left_join
d <- left_join(merged.data, activity.labels, by=c("y" = "activity.id"))
d <- join(merged.data, activity.labels, by=c("y" = "activity.id"))
d <- inner_join(merged.data, activity.labels, by=c("y" = "activity.id"))
names(activity.labels) <- c("activity.id", "activity.name")
d <- left_join(merged.data, activity.labels, by=c("y" = "activity.id"))
View(d)
merged.y.data <- left_join(merged.y.data, activity.labels, by=c("y" = "activity.id"))
View(merged.y.data)
#install.packages("dplyr")
library(dplyr)
setwd("C:\\ds\\git\\datasciencecoursera\\getting_and_cleaning_data\\project")
test.dir <- ".\\uci_har_dataset\\test\\"
train.dir <- ".\\uci_har_dataset\\train\\"
# You should create one R script called run_analysis.R that does the following.
# 1. Merges the training and the test sets to create one data set.
test.subject = read.table(paste0(test.dir,"subject_test.txt"))
test.x = read.table(paste0(test.dir,"x_test.txt"))
test.y <- read.table(paste0(test.dir, "y_test.txt"))
train.subject <- read.table(paste0(train.dir,"subject_train.txt"))
train.x <- read.table(paste0(train.dir, "x_train.txt"))
train.y <- read.table(paste0(train.dir, "y_train.txt"))
features.txt <- read.table(".\\uci_har_dataset\\features.txt")
merged.x.data <- rbind(test.x, train.x)
# 4. Appropriately labels the data set with descriptive variable names.
names(merged.x.data) <- features.txt[,2]
# 2. Extracts only the measurements on the mean and standard deviation for each measurement.
merged.x.data <- merged.x.data[, grepl("mean()", names(merged.x.data)) | grepl("std()", names(merged.x.data))]
merged.y.data <- rbind(test.y, train.y)
names(merged.y.data) <- "y"
# 3. Uses descriptive activity names to name the activities in the data set
activity.labels <- read.table(".\\uci_har_dataset\\activity_labels.txt")
names(activity.labels) <- c("activity.id", "activity.name")
merged.y.data <- left_join(merged.y.data, activity.labels, by=c("y" = "activity.id"))
merged.subject.data <- rbind(test.subject, train.subject)
names(merged.subject.data) <- "subject"
merged.data <- cbind(merged.subject.data, merged.y.data, merged.x.data)
names(merged.data)
View(merged.data)
?summarize
?summarize_each
?matches
averaged.data <- merged.data %>% group_by(subject, y) %>% summarize_each(funs(mean), matches(c("std", "mean")))
averaged.data <- merged.data %>% group_by(subject, y) %>% summarize_each(funs(mean), matches("std", "mean"))
View(averaged.data)
View(merged.data)
#install.packages("dplyr")
library(dplyr)
setwd("C:\\ds\\git\\datasciencecoursera\\getting_and_cleaning_data\\project")
test.dir <- ".\\uci_har_dataset\\test\\"
train.dir <- ".\\uci_har_dataset\\train\\"
# You should create one R script called run_analysis.R that does the following.
# 1. Merges the training and the test sets to create one data set.
#### OBJECTIVE 1: merge test and training sets into a single data set ####
## MERGE THE SUBJECT DATA FROM TEST AND TRAINING SETS ##
test.subject = read.table(paste0(test.dir,"subject_test.txt"))
train.subject <- read.table(paste0(train.dir,"subject_train.txt"))
subject.data <- rbind(test.subject, train.subject)
names(merged.subject.data) <- "subject"
names(subject.data) <- "subject"
rm(train.subject, test.subject)
View(subject.data)
test.y <- read.table(paste0(test.dir, "y_test.txt"))
train.y <- read.table(paste0(train.dir, "y_train.txt"))
activity.data <- rbind(test.y, train.y)
rm(test.y, train.y)
names(activity.data) <- "activity.id"
View(activity.data)
## MERGE THE MEASUREMENTS DATA (X DATA) FROM TEST AND TRAINING SETS ##
test.x = read.table(paste0(test.dir,"x_test.txt"))
train.x <- read.table(paste0(train.dir, "x_train.txt"))
measurement.data <- rbind(test.x, train.x)
rm(test.x, train.x)
features.txt <- read.table(".\\uci_har_dataset\\features.txt")
## set the measurement data columns based on variable names (in 2nd column) ##
names(measurement.data) <- features.txt[,2]
View(measurement.data)
#### OBJECTIVE 2: Extracts only the measurements on the mean and standard deviation for each measurement. ####
measurement.data <- measurement.data[, grepl("mean()", names(measurement.data)) | grepl("std()", names(measurement.data))]
activity.labels <- read.table(".\\uci_har_dataset\\activity_labels.txt")
names(activity.labels) <- c("activity.id", "activity.name")
activity.data <- left_join(activity.data, activity.labels, by="activity.id")
View(activity.data)
all.data <- cbind(subject.data, activity.data, measurement.data)
names(all.data)
#### OBJECTIVE 5: From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject. ####
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), matches("std", "mean"))
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), -names(all.data)[1:2])
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), matches(names(all.data)[3:82]))
?split
?paste
paste(names(all.data)[3:82],sep=",")
paste(names(all.data)[4:82],sep=",", collapse=TRUE)
paste(names(all.data)[4:82],sep=",", collapse)
paste(names(all.data)[4:82], collapse=",")
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), matches(paste(names(all.data)[4:82], collapse=",")))
?summarize_each
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), matches(paste(quote(names(all.data)[4:82]), collapse=",")))
quote(names(all.data)[4:82])
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), names(all.data)[-1:3])
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), names(all.data)[-(1:3)])
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), names(all.data)[-c(1:3)])
averaged.data <- all.data %>% group_by(subject, activity.id) %>% summarize_each(funs(mean), names(all.data)[-c(1,2,3)])
